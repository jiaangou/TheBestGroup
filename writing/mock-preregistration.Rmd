---
title: 'Spatio-temporal variation of benthic phototrophic elemental signatures along depth profiles'
author:
- name: Kirsten Bevandick
  affiliation: 2
- name: Kate Colson
  affiliation: 1
- name: Stefano Mezzini
  affiliation: 2
- name: William Ou
  affiliation: 1
- name: Julee Stewart
  affiliation: 3
date: '2021-10-06'
output: prereg::cos_prereg
affiliation:
- id: 1
  institution: University of British Columbia
- id: 2
  institution: University of British Columbia (Okanagan campus)
- id: 3
  institution: University of Regina
bibliography: prod-reprod-class-assignment-2.bib
csl: freshwater-biology.csl
shorttitle: ''
---

<!--
# Roles

1.  **Research question + hypothesis** (William)

    -   clearly identify the research question of interest in the replication?

    -   include at least 1 testable hypothesis?

2.  **Data** (Julee, Kate, Kirsten)

    -   Description of existing data and/or data collection procedures? (If existing data is included, is the reference(s) to the original data source included?)

    -   A description of the variables included in the dataset and/or to be included in the analysis

    -   A study design plan? (Kirsten)

3.  **Analysis**

    -   Does the pre-registration include at least 1 example statistical analysis using simulated/dummy data?

    -   Are the simulated data informed by published data?

4.  **Figure** (William)

    -   Does the pre-registration include a figure?

    -   summarize/present the key variables in the analysis (with appropriate response and predictor variables)?

    -   Include properly labelled axes and a legend (if applicable)?

    -   Include a figure caption

5.  **Literature** (Julee)

    -   Does the pre-registration include in-text citations and a bibliography of all studies mentioned?
-->
# Study Information

## Title

```{=html}
<!-- Provide the working title of your study. It may be the same title that you submit for publication of your final manuscript, but it is not a requirement. The title should be a specific and informative description of a project. Vague titles such as 'Fruit fly preregistration plan' are not appropriate.

Example: Effect of sugar on brownie tastiness. -->
```
`r rmarkdown::metadata$title`

## Description

Whole-lake ecosystem processes are complex and hard to quantify. Carbon and nitrogen cycling depend on the composition and abundance of a aquatic organisms, which, in turn, can vary spatio-temporally due to many physical and chemical factors. While most studies on whole-lake ecosystem processes focused focused primarily on pelagic production, @gushulak_effects_2021 presented the first study on nutrient cycling using depth profiles. Using stable isotope and $\color{red}{\text{nutrient}}$ concentration profiles of $\delta^{13}$C, $\delta^{15}$N, total C, total N, and the ratio of C to N, they determined that intermediate lake depths offered the best conditions for maximum phytobenthic production. In this study, our objective is to replicate the study by @gushulak_effects_2021 while also accounting for the variation between different locations of the lake.

## Hypotheses

Since light availability and disturbance due to currents and mixis affect phytobenthic production, we hypothesize that productivity will be highest at intermediate lake depths, where disturbance is low but the lake is shallow enough to allow large quantities of light to penetrate to the benthic region.

# Design Plan

## Study type

The proposed study is an observational study. Surface layer soil samples will be collected from various transects within the bed of Gall Lake. Isotopic signatures ($\delta^{13}$C, $\delta^{15}$N) and elemental composition (%C, %N, C:N) will be measured in each sample. For further details see the sampling plan and description of variables below.

## Blinding

Personnel who collect samples will be aware of the location, and depth the samples were collected at. Each sample will be labeled with a non-descriptive code to remove all information regarding the samples' locations and depths.

## Study design

Superficial soil samples (0-1 cm of depth) will be collected at 1-meter depth intervals (up to 16 m) in Gall Lake, Ontario, Canada, following four orthogonal transects (North, East, South, West). Four independent replicates will be performed along each transect, for a total of $16 \times 4 \times 4 = 256$ soil samples.

## Randomization

No randomization will be present in the study. To avoid anthropogenic sources of variation, all samples will be taken by a single individual on the same week. Samples will also be analyzed by a single technician in a single mass spectrometer.

# Sampling Plan

## Existing data

**Registration prior to creation of data**. As of the date of submission of this research plan for preregistration, the data have not yet been collected, created, or realized. All data presented here was simulated based on the data from @gushulak_effects_2021.

## Explanation of existing data

Existing data will not be used in our replication study; all data will be sampled after submitting the preregistration.

## Data collection procedures

```{=html}
<!-- Please describe the process by which you will collect your data. If you are using human subjects, this should include the population from which you obtain subjects, recruitment efforts, payment for participation, how subjects will be selected for eligibility from the initial pool (e.g. inclusion and exclusion rules), and your study timeline. For studies that donÍt include human subjects, include information about how you will collect samples, duration of data gathering efforts, source or location of samples, or batch numbers you will use.

The answer to this question requires a specific set of instructions so that another person could repeat the data collection procedures and recreate the study population. Alternatively, if the study population would be unable to be reproduced because it relies on a specific set of circumstances unlikely to be recreated (e.g., a community of people from a specific time and location), the criteria and methods for creating the group and the rationale for this unique set of subjects should be clear.

Example: Participants will be recruited through advertisements at local pastry shops. Participants will be paid $10 for agreeing to participate (raised to $30 if our sample size is not reached within 15 days of beginning recruitment). Participants must be at least 18 years old and be able to eat the ingredients of the pastries. -->
```

Data will be collected at the Gall Lake study site ($50^{\circ} 11''$ N, $90^{\circ} 42''$ W) located in Ontario, Canada. This undisturbed study site is surrounded by boreal forest with a high abundance of black spruce, jack pine, and poplar and a lower abundance of birch, balsam fir, and larch [@kingsbury_consistent_2012]. Data collection will take place in a single week during the summer of 2022.

Surface sediment samples (0-1cm) will be collected using a mini-Glew gravity coring apparatus [@glew_miniature_1991] as was done in @gushulak_effects_2021 at sequential depht intervals, determined using depth sounders [@gushulak_effects_2021]. The surface sediment sampling will be along three depth transects in each of the basins of Gall Lake, to expand upon the area covered in @gushulak_effects_2021 and account for any differences in deposition to the surface sediments due to currents, input, and mixing in Gall Lake. This expansion of coverage will allow for spatial variation among basins, as well as for variation within basins, to be considered in the geochemical analysis of the replication study.

The stable isotope analysis from the surface sediment samples will be conducted using standard methods [@savage_distribution_2004; @bunting_regulation_2010] using Thermo Finnigan Delta V isotope ratio mass spectrometer that is equipped with a ConFlow IV dilution inlet system, to ensure consistency with the methods of @gushulak_effects_2021.

## Sample size

```{=html}
<!-- Describe the sample size of your study. How many units will be analyzed in the study? This could be the number of people, birds, classrooms, plots, interactions, or countries included. If the units are not individuals, then describe the size requirements for each unit. If you are using a clustered or multilevel design, how many units are you collecting at each level of the analysis? For some studies, this will simply be the number of samples or the number of clusters. For others, this could be an expected range, minimum, or maximum number.

Example: Our target sample size is 280 participants. We will attempt to recruit up to 320, assuming that not all will complete the total task. -->
```
There will be 4 surface sediment samples taken at each of the 16 depth intervals (1-2m, 2-3m, 3-4m, etc.) along the depth transects in each basin of Gall Lake. This will result in a sample size N=256 for our replication study.

## Sample size rationale

```{=html}
<!-- This could include a power analysis or an arbitrary constraint such as time, money, or personnel. This gives you an opportunity to specifically state how the sample size will be determined. A wide range of possible answers is acceptable; remember that transparency is more important than principled justifications. If you state any reason for a sample size upfront, it is better than stating no reason and leaving the reader to "fill in the blanks." Acceptable rationales include: a power analysis, an arbitrary number of subjects, or a number based on time or monetary constraints.

Example: We used the software program G*Power to conduct a power analysis. Our goal was to obtain .95 power to detect a medium effect size of .25 at the standard .05 alpha error probability. -->
```
We will be working in a Bayesian framework, such that a power analysis is not applicable. Our sample size was therefore determined due to contraints in time and budget.

## Stopping rule

```{=html}
<!-- If your data collection procedures do not give you full control over your exact sample size, specify how you will decide when to terminate your data collection. 

You may specify a stopping rule based on p-values only in the specific case of sequential analyses with pre-specified checkpoints, alphas levels, and stopping rules. Unacceptable rationales include stopping based on p-values if checkpoints and stopping rules are not specified. If you have control over your sample size, then including a stopping rule is not necessary, though it must be clear in this question or a previous question how an exact sample size is attained.

Example: We will post participant sign-up slots by week on the preceding Friday night, with 20 spots posted per week. We will post 20 new slots each week if, on that Friday night, we are below 320 participants. -->
```
We will sample each 1m depth interval (e.g. 1-2m, 2-3m, 3-4m, etc.) until we reach the deepest point of Gall Lake (\~17m; [@kingsbury_consistent_2012]).

# Variables

<!-- In this section you can describe all variables (both manipulated and measured variables) that will later be used in your confirmatory analysis plan. In your analysis plan, you will have the opportunity to describe how each variable will be used. If you have variables which you are measuring for exploratory analyses, you are not required to list them, though you are permitted to do so. -->

## Manipulated variables

```{=html}
<!-- Describe all variables you plan to manipulate and the levels or treatment arms of each variable. This is not applicable to any observational study. For any experimental manipulation, you should give a precise definition of each manipulated variable. This must include a precise description of the levels at which each variable will be set, or a specific definition for each categorical treatment. For example, “loud or quiet,” should instead give either a precise decibel level or a means of recreating each level. 'Presence/absence' or 'positive/negative' is an acceptable description if the variable is precisely described.

Example: We manipulated the percentage of sugar by mass added to brownies. The four levels of this categorical variable are: 15%, 20%, 25%, or 40% cane sugar by mass. -->
```
#### Depth

For phytobenthos analyses, we will manipulate the depth that surface sediment samples are collected. Four sediment samples will be collected for every 1-m depth interval of Gall Lake. Samples for this categorical variable will range from a 1-m to 16-m depth and will be determined with the use of a depth sounder [@gushulak_effects_2021]. In R scripts, this manipulated variable will be named `depth_m`.

## Measured variables

```{=html}
<!-- Describe each variable that you will measure. This will include outcome measures, as well as any predictors or covariates that you will measure. You do not need to include any variables that you plan on collecting if they are not going to be included in the confirmatory analyses of this study.

Observational studies and meta-analyses will include only measured variables. As with the previous questions, the answers here must be precise. For example, 'intelligence,' 'accuracy,' 'aggression,' and 'color' are too vague. Acceptable alternatives could be 'IQ as measured by Wechsler Adult Intelligence Scale' 'percent correct,' 'number of threat displays,' and 'percent reflectance at 400 nm.'

Example: The single outcome variable will be the perceived tastiness of the single brownie each participant will eat. We will measure this by asking participants ‘How much did you enjoy eating the brownie’ (on a scale of 1-7, 1 being 'not at all', 7 being 'a great deal') and 'How good did the brownie taste' (on a scale of 1-7, 1 being 'very bad', 7 being 'very good'). -->
```
#### Stable Isotopic Analysis

The single outcome variables for the stable isotopic ratio of nitrogen and carbon will be measured from freeze-dried sediment subsamples of Gall Lake. Samples will be placed in a Thermo Finnigan Delta V isotope ratio mass spectrometer that has a ConFlow IV dilution inlet system as described by @gushulak_effects_2021 Following the calibration procedure for laboratory standards explained in @bunting_regulation_2010 and @savage_distribution_2004 , the isotope values will be analyzed with the use of atmospheric gas. The standard notation for the stable isotopic ratio of nitrogen and carbon are $\delta^{15}$N and $\delta^{13}$C, respectively. In R scripts, these single outcome variables will be named `d15n` and `d13c` for nitrogen and carbon, respectively.

## Indices

```{=html}
<!-- If any measurements are  going to be combined into an index (or even a mean), what measures will you use and how will they be combined? Include either a formula or a precise description of your method. If your are using a more complicated statistical method to combine measures (e.g. a factor analysis), you can note that here but describe the exact method in the analysis plan section.

If you are using multiple pieces of data to construct a single variable, how will this occur? Both the data that are included and the formula or weights for each measure must be specified. Standard summary statistics, such as "means" do not require a formula, though more complicated indices require either the exact formula or, if it is an established index in the field, the index must be unambiguously defined. For example, "biodiversity index" is too broad, whereas "Shannon’s biodiversity index" is appropriate.

Example: We will take the mean of the two questions above to create a single measure of 'brownie enjoyment.'  -->
```
#### Nitrogen and Carbon Content

Using generalized additive models, we will be determining the percent content of nitrogen and carbon in the depth intervals of Gall lake by manipulating the stable isotope content of these elements. Specifically, gamma distributions as described by @mushet_bottom-up_2020 will be used to determine the percent content. In R scripts, the manipulated variables will be named `perc_n` and `perc_c` for nitrogen and carbon, respectively.

*equation?*

# Analysis Plan

```{=html}
<!-- You may describe one or more confirmatory analysis in this preregistration. Please remember that all analyses specified below must be reported in the final article, and any additional analyses must be noted as exploratory or hypothesis generating.

A confirmatory analysis plan must state up front which variables are predictors (independent) and which are the outcomes (dependent), otherwise it is an exploratory analysis. You are allowed to describe any exploratory work here, but a clear confirmatory analysis is required. -->
```
## Statistical models

```{=html}
<!-- What statistical model will you use to test each hypothesis? Please include the type of model (e.g. ANOVA, multiple regression, SEM, etc) and the specification of the model (this includes each variable that will be included as predictors, outcomes, or covariates). Please specify any interactions, subgroup analyses, pairwise or complex contrasts, or follow-up tests from omnibus tests. If you plan on using any positive controls, negative controls, or manipulation checks you may mention that here. Remember that any test not included here must be noted as an exploratory test in your final article.

This is perhaps the most important and most complicated question within the preregistration. As with all of the other questions, the key is to provide a specific recipe for analyzing the collected data. Ask yourself: is enough detail provided to run the same analysis again with the information provided by the user? Be aware for instances where the statistical models appear specific, but actually leave openings for the precise test. See the following examples:

- If someone specifies a 2x3 ANOVA with both factors within subjects, there is still flexibility with the various types of ANOVAs that could be run. Either a repeated measures ANOVA (RMANOVA) or a multivariate ANOVA (MANOVA) could be used for that design, which are two different tests. 
- If you are going to perform a sequential analysis and check after 50, 100, and 150 samples, you must also specify the p-values you’ll test against at those three points.

Example:  We will use a one-way between subjects ANOVA to analyze our results. The manipulated, categorical independent variable is 'sugar' whereas the dependent variable is our taste index. -->
```
Similarly to @gushulak_effects_2021, the isotope data was analyzed using Generalized Additive Models (GAMs) via the `mgcv` package [@wood_fast_2011; @wood_generalized_2017]. The amount of $^{15}$N and $^13$C (`d15n` and `d13c` in the `R` scripts, respectively) were modeled using a Gaussian conditional distribution with an *identity* link function since both parameters take any real value (i.e. they can be both positive and negative). The percentages of nitrogen and carbon in the samples (`perc_n` and `perc_c`, respectively) were converted to proportions (`frac_n` and `frac_c`) so they coould be modeled with a beta distribution with a *logit* link function. (There is no distribution in the `mgcv` package for numbers between 0 and 100). Finally, the proportion of carbon to nitrogen (C:N, `c_n_ratio` in the scripts) were modeled using a GAM with a gamma conditional distribution and a *log* link function. A gamma distribution was most appropriate since the ratio of two positive (non-zero) numbers is strictly greater than zero. The mean effects and their Bayesian credible intervals (CIs) were estimated on the link scale using normal approximation ($\pm 1.96$ standard deviations) and back-transformed to response values using the inverse-link function. For example, the mean C:N ratios and their CIs was estimated on the *log* scale and then back-transformed to ratios by exponentiating the estimate. (See the following section for more information on GAMs and transformations.)

All five models accounted for a shared global trend between replicates and trends within-replicate. For simplicity, the smoothness parameter was assumed to be the same between replicates [see model "GS" in @pedersen_hierarchical_2019].

```{r models, eval = FALSE}
# stable nitrogen isotope
m_d15n <- gam(d15n ~
                s(depth_m, k = 15) + # average effect of depth
                s(depth_m, replicate, k = 10, bs = 'fs'), # effect of replicate
              family = gaussian(link = 'identity'), # I(k) = k
              data = isotopes,
              method = 'REML') # optimiziation method for the smoothness parameter

# stable carbon isotope
m_d13c <- gam(d13c ~
                s(depth_m, k = 15) +
                s(depth_m, replicate, k = 10, bs = 'fs'),
              family = gaussian(link = 'identity'),
              data = isotopes,
              method = 'REML')

# fraction of carbon in the samples
m_frac_c <- gam(frac_c ~
                  s(depth_m, k = 15) +
                  s(depth_m, replicate, k = 10, bs = 'fs'),
                family = betar(link = 'logit'), # logit(0) = -Inf, logit(1) = Inf
                data = isotopes,
                method = 'REML')

# fraction of nitrogen in the samples
m_frac_n <- gam(frac_n ~
                  s(depth_m, k = 15) +
                  s(depth_m, replicate, k = 10, bs = 'fs'),
                family = betar(link = 'logit'),
                data = isotopes,
                method = 'REML')

#C:N ratio in the samples
m_c_n <- gam(c_n_ratio ~
               s(depth_m, k = 15) +
               s(depth_m, replicate, k = 10, bs = 'fs'),
             family = Gamma(link = 'log'), # log(0) = -Inf, log(Inf) = Inf
             data = isotopes,
             method = 'REML')
```

Thus, each model had two predictors: a predictor which accounted for the mean effect of lake depth (`s(depth_m, k = 15)`) and one which used factor smooths (`bs = 'fs'`) for each replicate. Both of the predictors used thin plate regression splines (the default for the `s()` function in `mgcv`). The number of knots (`k`) for each smooth was allowed to be reasonably high, since the models were fit via penalized maximum likelihood and thus over-fitting the data was unlikely [@wood_fast_2011; @simpson_modelling_2018]. The smoothness parameters were optimized using Restricted Marginal Likelihood (`method = 'REML'`), rather than the default Generalized Cross Validation (GCV), since REML does not over-fit as often as GCV [@reiss_smoothing_2009; @wood_fast_2011].

Although credible intervals were estimated, this study was performed with a purely Bayesian approach, so there was no interest in statistical significance and Frequentist null-hypothesis testing. Rather, the models were used to compare similarities between our estimates and the estimates by @gushulak_effects_2021.

## Transformations {#transformations}

```{=html}
<!-- If you plan on transforming, centering, recoding the data, or will require a coding scheme for categorical variables, please describe that process. If any categorical predictors are included in a regression, indicate how those variables will be coded (e.g. dummy coding, summation coding, etc.) and what the reference category will be.

Example: The "Effect of sugar on brownie tastiness" does not require any additional transformations. However, if it were using a regression analysis and each level of sweet had been categorically described (e.g. not sweet, somewhat sweet, sweet, and very sweet), 'sweet' could be dummy coded with 'not sweet' as the reference category. -->
```
The only data transformation that was performed was the conversion of percent carbon and nitrogen (between 0% and 100%) to proportions (between 0 and 1). Note that since the transformation $a = \frac{b}{100}$ is a linear transformation, Jensen's inequality does not apply here [@jensen_sur_1906].

Modeling the data with GAMs removes the need for transforming data which violates the normality assumptions which linear models depend on: GAMs estimate transformed mean responses ($g[\mathbb E(Y)]$, where $g(\cdot)$ is the link function) rather than the mean transformed response ($\mathbb E[g(Y)]$), so Jensen's inequality [@jensen_sur_1906] does not apply here. \#\# Inference criteria

```{=html}
<!-- What criteria will you use to make inferences? Please describe the information youÍll use (e.g. p-values, bayes factors, specific model fit indices), as well as cut-off criterion, where appropriate. Will you be using one or two tailed tests for each of your analyses? If you are comparing multiple conditions or testing multiple hypotheses, will you account for this?

p-values, confidence intervals, and effect sizes are standard means for making an inference, and any level is acceptable, though some criteria must be specified in this or previous fields. Bayesian analyses should specify a Bayes factor or a credible interval. If you are selecting models, then how will you determine the relative quality of each? In regards to multiple comparisons, this is a question with few "wrong" answers. In other words, transparency is more important than any specific method of controlling the false discovery rate or false error rate. One may state an intention to report all tests conducted or one may conduct a specific correction procedure; either strategy is acceptable.

Example: We will use the standard p<.05 criteria for determining if the ANOVA and the post hoc test suggest that the results are significantly different from those expected if the null hypothesis were correct. The post-hoc Tukey-Kramer test adjusts for multiple comparisons. -->
```
## Data exclusion

```{=html}
<!-- How will you determine what data or samples, if any, to exclude from your analyses? How will outliers be handled? Will you use any awareness check? Any rule for excluding a particular set of data is acceptable. One may describe rules for excluding a participant or for identifying outlier data.

Example: No checks will be performed to determine eligibility for inclusion besides verification that each subject answered each of the three tastiness indices. Outliers will be included in the analysis. -->
```
Enter your response here.

## Missing data

```{=html}
<!-- How will you deal with incomplete or missing data? Any relevant explanation is acceptable. As a final reminder, remember that the final analysis must follow the specified plan, and deviations must be either strongly justified or included as a separate, exploratory analysis.

Example: If a subject does not complete any of the three indices of tastiness, that subject will not be included in the analysis. -->
```
Enter your response here.

## Exploratory analyses (optional)

```{=html}
<!-- If you plan to explore your data set to look for unexpected differences or relationships, you may describe those tests here. An exploratory test is any test where a prediction is not made up front, or there are multiple possible tests that you are going to use. A statistically significant finding in an exploratory test is a great way to form a new confirmatory hypothesis, which could be registered at a later time.

Example: We expect that certain demographic traits may be related to taste preferences. Therefore, we will look for relationships between demographic variables (age, gender, income, and marital status) and the primary outcome measures of taste preferences. -->
```
N/A

# Other

## Other (Optional)

<!-- If there is any additional information that you feel needs to be included in your preregistration, please enter it here. Literature cited, disclosures of any related work such as replications or work that uses the same data, or other context that will be helpful for future readers would be appropriate here. -->

Enter your response here.

# References

## 

```{=tex}
\vspace{-2pc}
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{-1in}
\setlength{\parskip}{8pt}
```
\noindent
